{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s4EFyIfdJ77"
      },
      "outputs": [],
      "source": [
        "# 1_load_dataset.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_dataset(csv_file=\"public_services_asr_manifest.csv\"):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    print(\"✅ Dataset loaded:\", df.shape)\n",
        "    print(df.head(5))\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2_preprocess_clean.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def clean_dataset(df):\n",
        "    df.dropna(subset=[\"transcription\", \"audio_filepath\"], inplace=True)\n",
        "    df.drop_duplicates(subset=[\"language\", \"transcription\"], inplace=True)\n",
        "    print(\"✅ After cleaning:\", df.shape)\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_dataset()\n",
        "    df = clean_dataset(df)\n",
        "    print(df.head(5))\n"
      ],
      "metadata": {
        "id": "pmvpl5Rp_vVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3_preprocess_normalize.py\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "def normalize_text(text):\n",
        "    text = str(text)\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def normalize_dataset(df):\n",
        "    df[\"transcription\"] = df[\"transcription\"].map(normalize_text)\n",
        "    print(\"✅ After normalization sample:\")\n",
        "    print(df[[\"language\", \"transcription\"]].head(5))\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_dataset()\n",
        "    df = clean_dataset(df)\n",
        "    df = normalize_dataset(df)\n"
      ],
      "metadata": {
        "id": "xCe-MGfm_1NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4_preprocess_split.py\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_dataset(df):\n",
        "    train, temp = train_test_split(df, test_size=0.2, stratify=df[\"language\"], random_state=42)\n",
        "    val, test = train_test_split(temp, test_size=0.5, stratify=temp[\"language\"], random_state=42)\n",
        "\n",
        "    print(\"✅ Train size:\", len(train))\n",
        "    print(\"✅ Validation size:\", len(val))\n",
        "    print(\"✅ Test size:\", len(test))\n",
        "    return train, val, test\n",
        "\n",
        "def save_manifests(train, val, test, output_dir=\"manifests\"):\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "    for split_name, split_df in {\"train\": train, \"validation\": val, \"test\": test}.items():\n",
        "        out_path = Path(output_dir) / f\"{split_name}_manifest.jsonl\"\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for _, row in split_df.iterrows():\n",
        "                f.write(json.dumps({\n",
        "                    \"audio_filepath\": row[\"audio_filepath\"],\n",
        "                    \"transcription\": row[\"transcription\"],\n",
        "                    \"language\": row[\"language\"]\n",
        "                }, ensure_ascii=False) + \"\\n\")\n",
        "        print(f\"✅ {split_name} manifest saved at {out_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_dataset()\n",
        "    df = clean_dataset(df)\n",
        "    df = normalize_dataset(df)\n",
        "    train, val, test = split_dataset(df)\n",
        "    save_manifests(train, val, test)\n"
      ],
      "metadata": {
        "id": "1UYuoHa1_5xg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
